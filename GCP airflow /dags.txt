# import all modules


import airflow
from airflow import DAG
from datetime import timedelta
from airflow.utils.dates import days_ago
from airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator
from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator

# default arguments / variables


PROJECT_ID = "swift-park-474909-f8"
SOURCE_BUCKET = "comp-dag-test"
DATASET_NAME_1 = "raw_ds"
DATASET_NAME_2 = "insight_ds"
TABLE_NAME_1 = "emp_raw"
TABLE_NAME_2 = "dep_raw"
TABLE_NAME_3 = "empDep_in"

ARGS = {
    'owner': 'Ayaan',
    'start_date': days_ago(1),
    'depends_on_past': False,
    'email_on_failure': True,
    'email_on_retry': True,
    'email_on_success': True,
    'email': ['reddyayaan74@gmail.com'],
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

# query

QUERY = f"""
CREATE OR REPLACE TABLE `{PROJECT_ID}.{DATASET_NAME_2}.{TABLE_NAME_3}` AS
SELECT
    e.EmployeeID,
    CONCAT(e.FirstName, ' ', e.LastName) AS FullName,
    e.Email,
    e.DepartmentID,
    e.Salary,
    e.JoinDate,
    d.DepartmentName,
    CAST(e.Salary AS INTEGER) * 0.01 AS Tax
FROM
    `{PROJECT_ID}.{DATASET_NAME_1}.{TABLE_NAME_1}` AS e
JOIN
    `{PROJECT_ID}.{DATASET_NAME_1}.{TABLE_NAME_2}` AS d
ON
    e.DepartmentID = d.DepartmentID
WHERE
    e.EmployeeID IS NOT NULL
"""

# define the DAG


with DAG(
    dag_id="level_1_dag",
    schedule_interval="0 5 * * *", # this is corn job formate 
    description='DAG to load data from GCS to BigQuery and create an enriched employee table',
    default_args=ARGS,
    tags=["marvel", "level_1", "gcs_to_bq"],
) as dag:


# here we are defineing operator u can google search that gcs to bigqeary operator 

    task_1 = GCSToBigQueryOperator(
        task_id="empTask",
        bucket=SOURCE_BUCKET,
        source_objects=["employee.csv"],
        destination_project_dataset_table=f"{DATASET_NAME_1}.{TABLE_NAME_1}",
        schema_fields=[
            {"name": "EmployeeID", "type": "INT64", "mode": "NULLABLE"},
            {"name": "FirstName", "type": "STRING", "mode": "NULLABLE"},
            {"name": "LastName", "type": "STRING", "mode": "NULLABLE"},
            {"name": "Email", "type": "STRING", "mode": "NULLABLE"},
            {"name": "DepartmentID", "type": "INT64", "mode": "NULLABLE"},
            {"name": "Salary", "type": "FLOAT64", "mode": "NULLABLE"},
            {"name": "JoinDate", "type": "STRING", "mode": "NULLABLE"},
        ],
        write_disposition="WRITE_TRUNCATE",
    )

    task_2 = GCSToBigQueryOperator(
        task_id="depTask",
        bucket=SOURCE_BUCKET,
        source_objects=["departments.csv"],
        destination_project_dataset_table=f"{DATASET_NAME_1}.{TABLE_NAME_2}",
        schema_fields=[
            {"name": "DepartmentID", "type": "INT64", "mode": "NULLABLE"},
            {"name": "DepartmentName", "type": "STRING", "mode": "NULLABLE"},
        ],
        write_disposition="WRITE_TRUNCATE",
    )

# here also same u have to define operator for queary  

    task_3 = BigQueryInsertJobOperator(
        task_id="empDepTask",
        configuration={
            "query": {
                "query": QUERY,
                "useLegacySql": False,
                "priority": "BATCH",
            }
        },
    )

    # task dependencies
    [task_1, task_2] >> task_3    
