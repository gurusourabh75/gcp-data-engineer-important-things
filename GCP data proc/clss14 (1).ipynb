{"cells": [{"cell_type": "code", "execution_count": 1, "id": "ebbe286a-a22a-4351-ba2e-e292c9690c02", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Requirement already satisfied: pyspark in /usr/lib/spark/python (3.5.3)\nRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/miniconda3/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m"}], "source": "#  installing pyspark frame work \n\n!pip install pyspark "}, {"cell_type": "code", "execution_count": 2, "id": "b385a8e0-ff3d-40d0-abde-5749422188ef", "metadata": {"tags": []}, "outputs": [], "source": "## import data all files ------------------------------------------------------------------------------------------"}, {"cell_type": "code", "execution_count": 3, "id": "538a3173-066f-4427-9c6e-c52ddb0d37ea", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "26/02/10 08:07:44 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "# creating spark session \n\nimport pyspark \nfrom pyspark.sql import SparkSession\n\nspark=(SparkSession.builder.master(\"local\").appName(\"demo\").getOrCreate())"}, {"cell_type": "code", "execution_count": 4, "id": "a0e161e0-b6c2-4a23-8d8d-1982140dd50c", "metadata": {"tags": []}, "outputs": [{"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://cluster-59a1-m.us-east1-d.c.regular-use-486315.internal:33395\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.5.3</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7f3c23015190>"}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": "spark"}, {"cell_type": "code", "execution_count": 5, "id": "b3002cb9-8f7f-4fe2-89ee-3def431e46eb", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+---+-------+\n| id|   name|\n+---+-------+\n|  1|   anil|\n|  2|sandeep|\n|  3|   john|\n|  4|   baby|\n+---+-------+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n\n"}], "source": "emp = [(1, 'anil'),\n       (2, 'sandeep'),\n       (3, 'john'),\n       (4, 'baby')]\n\ncolumns = [\"id\", \"name\"]\n\n# creating data frame \n\ndf1=spark.createDataFrame(data=emp,schema=columns)\ndf1.show()\ndf1.printSchema()"}, {"cell_type": "code", "execution_count": 6, "id": "8aa16bd3-78c2-42cd-94a2-a9f13004a17d", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 2:>                                                          (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-------------------------------+\n|value                          |\n+-------------------------------+\n|Product,Category,Price,Quantity|\n|Widget,Electronics,10.00,100   |\n|Gadget,Electronics,15.00,75    |\n|Book,Books,5.00,200            |\n|Toy,Toys,8.00,150              |\n+-------------------------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# import text file \n\ntext_source_path=\"gs://class14-files/products.txt\"\n\ndf2=spark.read.text(text_source_path)\ndf2.show(truncate=False)"}, {"cell_type": "code", "execution_count": 7, "id": "cb3a3d0c-ef0e-458b-a6d6-b9620a1597ea", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+----------+-------------+-------+-----+\n|      Date|         City|Product|Sales|\n+----------+-------------+-------+-----+\n|2023-01-01|     New York| Widget|  100|\n|2023-01-02|     New York| Widget|  150|\n|2023-01-01|  Los Angeles| Gadget|  200|\n|2023-01-02|  Los Angeles| Gadget|  250|\n|2023-01-01|San Francisco| Widget|  120|\n|2023-01-02|San Francisco| Widget|  130|\n+----------+-------------+-------+-----+\n\nroot\n |-- Date: date (nullable = true)\n |-- City: string (nullable = true)\n |-- Product: string (nullable = true)\n |-- Sales: integer (nullable = true)\n\n"}], "source": "# IMPORTING CSV \n\ncsv_source_path=\"gs://class14-files/sales.csv\"\n\ndf3=spark.read.csv(csv_source_path,header=True, inferSchema=True)\ndf3.show()\ndf3.printSchema()"}, {"cell_type": "code", "execution_count": 8, "id": "47479d54-8329-4f59-8417-4e40341d3982", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+-------+\n|age|   name|\n+---+-------+\n| 28|  Alice|\n| 24|    Bob|\n| 32|Charlie|\n+---+-------+\n\nroot\n |-- age: long (nullable = true)\n |-- name: string (nullable = true)\n\n"}], "source": "# importing json file from source \n\njson_source_path=\"gs://class14-files/employee_data (1).json\"\n\ndf4=spark.read.json(json_source_path)\ndf4.show()\ndf4.printSchema()"}, {"cell_type": "code", "execution_count": 9, "id": "ee39e17c-a6a3-4332-ba16-0c45959dd765", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n|              model| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|\n+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n|          Mazda RX4|21.0|  6|160.0|110| 3.9| 2.62|16.46|  0|  1|   4|   4|\n|      Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|2.875|17.02|  0|  1|   4|   4|\n|         Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|\n|     Hornet 4 Drive|21.4|  6|258.0|110|3.08|3.215|19.44|  1|  0|   3|   1|\n|  Hornet Sportabout|18.7|  8|360.0|175|3.15| 3.44|17.02|  0|  0|   3|   2|\n|            Valiant|18.1|  6|225.0|105|2.76| 3.46|20.22|  1|  0|   3|   1|\n|         Duster 360|14.3|  8|360.0|245|3.21| 3.57|15.84|  0|  0|   3|   4|\n|          Merc 240D|24.4|  4|146.7| 62|3.69| 3.19| 20.0|  1|  0|   4|   2|\n|           Merc 230|22.8|  4|140.8| 95|3.92| 3.15| 22.9|  1|  0|   4|   2|\n|           Merc 280|19.2|  6|167.6|123|3.92| 3.44| 18.3|  1|  0|   4|   4|\n|          Merc 280C|17.8|  6|167.6|123|3.92| 3.44| 18.9|  1|  0|   4|   4|\n|         Merc 450SE|16.4|  8|275.8|180|3.07| 4.07| 17.4|  0|  0|   3|   3|\n|         Merc 450SL|17.3|  8|275.8|180|3.07| 3.73| 17.6|  0|  0|   3|   3|\n|        Merc 450SLC|15.2|  8|275.8|180|3.07| 3.78| 18.0|  0|  0|   3|   3|\n| Cadillac Fleetwood|10.4|  8|472.0|205|2.93| 5.25|17.98|  0|  0|   3|   4|\n|Lincoln Continental|10.4|  8|460.0|215| 3.0|5.424|17.82|  0|  0|   3|   4|\n|  Chrysler Imperial|14.7|  8|440.0|230|3.23|5.345|17.42|  0|  0|   3|   4|\n|           Fiat 128|32.4|  4| 78.7| 66|4.08|  2.2|19.47|  1|  1|   4|   1|\n|        Honda Civic|30.4|  4| 75.7| 52|4.93|1.615|18.52|  1|  1|   4|   2|\n|     Toyota Corolla|33.9|  4| 71.1| 65|4.22|1.835| 19.9|  1|  1|   4|   1|\n+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+\nonly showing top 20 rows\n\nroot\n |-- model: string (nullable = true)\n |-- mpg: double (nullable = true)\n |-- cyl: integer (nullable = true)\n |-- disp: double (nullable = true)\n |-- hp: integer (nullable = true)\n |-- drat: double (nullable = true)\n |-- wt: double (nullable = true)\n |-- qsec: double (nullable = true)\n |-- vs: integer (nullable = true)\n |-- am: integer (nullable = true)\n |-- gear: integer (nullable = true)\n |-- carb: integer (nullable = true)\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# import parquet file \n \n    \npar_source_path=\"gs://class14-files/MT_cars.parquet\"\ndf5=spark.read.parquet(par_source_path,header=True, inferSchema=True)\ndf5.show()\ndf5.printSchema()\n"}, {"cell_type": "code", "execution_count": 10, "id": "42342155-5392-4760-867d-259dc587db6f", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 10:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+----+--------------------+--------------------+----------------+--------------------+--------------------+--------------------+--------------------+-----------------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+------------+--------------+\n|        playlist_url|year|            track_id|          track_name|track_popularity|               album|           artist_id|         artist_name|       artist_genres|artist_popularity|danceability|energy|key|loudness|mode|speechiness|acousticness|instrumentalness|liveness|  tempo|duration_min|time_signature|\n+--------------------+----+--------------------+--------------------+----------------+--------------------+--------------------+--------------------+--------------------+-----------------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+------------+--------------+\n|https://open.spot...|2010|6FSxwdN08PvzimGAp...|     When We Collide|              44|Letters (Deluxe E...|3906URNmNa1VCXEei...|         Matt Cardle|     ['talent show']|               29|       0.443| 0.683|  2|  -5.521|   1|     0.0343|      0.0198|         5.26E-6|   0.313| 81.986|        3.77|             4|\n|https://open.spot...|2003|1unSsa1sbyaVy1tei...|Sweet Dreams My L...|              48|          Funky Dory|7LP4hiTAVkw3Rtc1h...|      Rachel Stevens|['bubblegum dance...|               31|       0.852| 0.865|  2|  -3.586|   1|      0.139|      0.0788|         0.00136|  0.0919|130.022|        3.47|             4|\n|https://open.spot...|2002|5TWL2K9Q9QuUTW15I...|           Kiss Kiss|              55|          Footprints|7gRmesSjINzb4xXAp...|       Holly Valance|['australian pop'...|               34|       0.705| 0.717|  4|  -4.944|   0|      0.125|     0.00369|           0.461|  0.0701| 97.036|        3.41|             4|\n|https://open.spot...|2000|6znv7i4Wif5fLwI6O...|           Back Here|              52|     Sooner Or Later|7y1GT7SdgGiFLWokt...|               BBMAK|        ['boy band']|               35|       0.624| 0.945|  7|  -3.109|   1|     0.0343|      0.0698|         4.57E-6|  0.0867| 104.77|        3.64|             4|\n|https://open.spot...|2001|4Mnw05kNd0j1XDHwt...|Do You Really Lik...|              57|Do You Really Lik...|7KMddxPqjqv0fiucw...|DJ Pied Piper & T...|       ['uk garage']|               35|       0.847| 0.877|  2|  -5.424|   1|     0.0493|       0.124|             0.0|   0.148|131.044|        3.62|             4|\n|https://open.spot...|2000|487Dyzs7Xbbk4hIWY...| Don't Think I'm Not|              56|        Hey Kandi...|0ThrQemMA2B8xxusT...|               Kandi|['contemporary r&...|               36|       0.859| 0.622| 11|  -8.196|   1|     0.0445|      0.0661|             0.0|  0.0394|134.007|        4.06|             4|\n|https://open.spot...|2002|1IMbmQCnl3h4m8NXq...|       Just A Little|              54|    Thinking It Over|6htUPs3clIStnkvg5...|           Liberty X|['europop', 'girl...|               36|       0.796| 0.617|  5|  -6.198|   0|     0.0619|     0.00824|             0.0|  0.0495|103.858|        3.94|             4|\n|https://open.spot...|2002|2RLpFEf6d0708O7Bq...|          Days Go By|              52|         Dirty Vegas|2IkHcHKErbWa0TA14...|         Dirty Vegas|                  []|               37|       0.786| 0.853|  9|  -8.274|   0|     0.0688|      0.0499|          0.0872|    0.35| 126.99|         7.2|             4|\n|https://open.spot...|2005|0reKGVcWLVvl1FzjZ...|I Like The Way - ...|              56|         Bodyrockers|5GJmQjUNRyNQ2VZ4H...|         Bodyrockers|      ['dance rock']|               38|       0.642| 0.851|  6|  -3.638|   1|     0.0431|      0.0248|          0.0146|   0.084|127.988|        3.33|             4|\n|https://open.spot...|2001|1Sm3U3B5XoidYQEZQ...|Heaven Is a Halfp...|              58|  Menace To Sobriety|5X2XAU0eSvRPWUl9h...|                 OPM|        ['rap rock']|               38|       0.743| 0.894|  8|  -6.886|   1|     0.0349|      0.0755|         0.00283|   0.367|   95.9|        4.29|             4|\n|https://open.spot...|2007|68Zq7PMcPIQE8ol1x...|      The Way I Live|              60|    Across The Water|15Y1yUEchPPl4w20C...|  Baby Boy Da Prince|                  []|               39|        0.88| 0.438|  0|  -7.562|   1|      0.248|      0.0147|             0.0|  0.0907|  90.02|        5.42|             4|\n|https://open.spot...|2011|0IF7bHzCXCZoKNog5...|Wherever You Will Go|              60|Wherever You Will Go|0qk8MxMzgnfFECvDO...|     Charlene Soraia|['neo-singer-song...|               39|       0.597| 0.115|  9|  -9.217|   1|     0.0334|        0.82|         2.15E-4|   0.111|111.202|        3.29|             4|\n|https://open.spot...|2006|1JOxCh2Aa08z8cNJQ...|Fill My Little World|              51|Twelve Stops And ...|4AksvCnkZaQoTu1nJ...|         The Feeling|['english indie r...|               39|       0.462| 0.783|  7|  -7.814|   1|     0.0309|      0.0333|             0.0|   0.348|175.964|        4.12|             4|\n|https://open.spot...|2003|2T4rXDppGlcy2lLuF...|               Angel|              49|               Angel|5DqmNLPM1kAbSBQk2...|        Amanda Perez|['hip pop', 'r&b'...|               40|       0.638|  0.54|  0|  -6.849|   1|     0.0473|       0.506|             0.0|   0.157|143.772|        3.65|             4|\n|https://open.spot...|2007|6DbqCKweKwVkHgRv1...|Pop, Lock & Drop ...|              57|      Notebook Paper|33wbkdcxtduHKY53L...|                Huey|['dirty south rap...|               40|       0.723| 0.644| 11|  -6.863|   0|      0.222|     0.00308|             0.0|   0.352| 144.09|        4.35|             4|\n|https://open.spot...|2006|0tYPj0NYa7vjlJaql...|      Chain Hang Low|              59|   Jibbs feat. Jibbs|4USNIVeRwXIAdbVST...|               Jibbs| ['dirty south rap']|               40|       0.792| 0.589|  4|  -6.869|   0|      0.262|      0.0259|         1.28E-4|   0.114|157.147|        3.46|             4|\n|https://open.spot...|2010|1HfxPaJggVwFsvOtH...|Live Like We're D...|              53|          Kris Allen|2zwHaEmXxX6DTv4i8...|          Kris Allen|['acoustic pop', ...|               40|       0.589| 0.893|  0|  -2.948|   1|     0.0397|      0.0273|             0.0|   0.343| 92.011|        3.54|             4|\n|https://open.spot...|2001|0xikWgPgYN9BEes0i...| Heard It All Before|              61|          Your Woman|0hnmRa5ahunapQbPj...|   Sunshine Anderson|['contemporary r&...|               40|       0.697| 0.925|  3|  -4.209|   0|      0.216|      0.0614|             0.0|   0.095| 96.951|        4.93|             4|\n|https://open.spot...|2001|5zn1hpm9N0ylKB7kO...|No More (Baby I'm...|              57|                 3LW|2lFHVcUeJ9Gq6AZiU...|                 3LW|['contemporary r&...|               41|       0.721| 0.723|  2|   -7.08|   0|     0.0631|       0.102|          4.4E-6|  0.0651| 88.933|        4.39|             4|\n|https://open.spot...|2007|0U969xYNlAyfzi8P1...|        Wait for You|              61|       Elliott Yamin|4am1I89OWXUzFh4ct...|       Elliott Yamin|            ['idol']|               41|       0.764| 0.487|  0|  -6.734|   1|     0.0281|        0.25|             0.0|   0.184|116.027|        4.36|             4|\n+--------------------+----+--------------------+--------------------+----------------+--------------------+--------------------+--------------------+--------------------+-----------------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+------------+--------------+\nonly showing top 20 rows\n\nroot\n |-- playlist_url: string (nullable = true)\n |-- year: long (nullable = true)\n |-- track_id: string (nullable = true)\n |-- track_name: string (nullable = true)\n |-- track_popularity: long (nullable = true)\n |-- album: string (nullable = true)\n |-- artist_id: string (nullable = true)\n |-- artist_name: string (nullable = true)\n |-- artist_genres: string (nullable = true)\n |-- artist_popularity: long (nullable = true)\n |-- danceability: double (nullable = true)\n |-- energy: double (nullable = true)\n |-- key: long (nullable = true)\n |-- loudness: double (nullable = true)\n |-- mode: long (nullable = true)\n |-- speechiness: double (nullable = true)\n |-- acousticness: double (nullable = true)\n |-- instrumentalness: double (nullable = true)\n |-- liveness: double (nullable = true)\n |-- tempo: double (nullable = true)\n |-- duration_min: double (nullable = true)\n |-- time_signature: long (nullable = true)\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# reading data from bigquery \n\ntable_id=\"regular-use-486315.silver_region.spotify_silver\"\n\ndf6= (spark.read.format(\"bigquery\").option(\"table\",table_id).load())\ndf6.show()\ndf6.printSchema()"}, {"cell_type": "code", "execution_count": 11, "id": "58ea40e1-2173-4287-b103-a7b1018f3781", "metadata": {"tags": []}, "outputs": [], "source": "# data from mysql local to dataproc or pypark or biquery "}, {"cell_type": "code", "execution_count": 12, "id": "be71fd05-deae-4e39-85ab-6bb84e331588", "metadata": {"tags": []}, "outputs": [], "source": "\n# load data ------------------------------------------------------------------------------------------------------------------------------------------------------"}, {"cell_type": "code", "execution_count": 14, "id": "b21b14bd-5c51-4bbc-898d-df1877f6ddca", "metadata": {"tags": []}, "outputs": [], "source": "\n# load data back to storage"}, {"cell_type": "code", "execution_count": 15, "id": "d0b33119-a54b-4a3a-a644-302a79d6c708", "metadata": {"tags": []}, "outputs": [], "source": "# csv file to sink "}, {"cell_type": "code", "execution_count": 23, "id": "f7b4c42d-fb18-4757-9d57-b6923688360f", "metadata": {"tags": []}, "outputs": [{"ename": "AnalysisException", "evalue": "[PATH_ALREADY_EXISTS] Path gs://class14-files/csv1 already exists. Set mode as \"overwrite\" to overwrite the existing path.", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sink_source_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://class14-files/csv1/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43msink_source_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[1;32m   1863\u001b[0m )\n\u001b[0;32m-> 1864\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n", "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path gs://class14-files/csv1 already exists. Set mode as \"overwrite\" to overwrite the existing path."]}], "source": "sink_source_path=\"gs://class14-files/csv1/\"\ndf.write.csv(sink_source_path,header=True)"}, {"cell_type": "code", "execution_count": 18, "id": "c71f18c6-a258-4272-b177-519f9c0ced45", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "2"}, "execution_count": 18, "metadata": {}, "output_type": "execute_result"}], "source": "df.rdd.getNumPartitions()"}, {"cell_type": "code", "execution_count": 22, "id": "6770a315-7d4e-4bef-bdc5-2231e3d2919c", "metadata": {"tags": []}, "outputs": [{"ename": "AnalysisException", "evalue": "[PATH_ALREADY_EXISTS] Path gs://class14-files/csv1 already exists. Set mode as \"overwrite\" to overwrite the existing path.", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sink_source_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://class14-files/csv1/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43msink_source_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[1;32m   1863\u001b[0m )\n\u001b[0;32m-> 1864\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n", "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path gs://class14-files/csv1 already exists. Set mode as \"overwrite\" to overwrite the existing path."]}], "source": "sink_source_path=\"gs://class14-files/csv1/\"\ndf.coalesce(1).write.csv(sink_source_path,header=True)"}, {"cell_type": "code", "execution_count": null, "id": "2ad0ffd0-861f-476e-a101-446178adffde", "metadata": {}, "outputs": [], "source": "\n"}, {"cell_type": "code", "execution_count": 24, "id": "173d7be7-3279-49d8-8e9b-6696a85f30de", "metadata": {"tags": []}, "outputs": [], "source": "\n#  josan file to sink"}, {"cell_type": "code", "execution_count": 26, "id": "7b08c1d9-e2bb-4338-86af-ffe050c8bfc7", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "\njson_sink_path=\"gs://class14-files/json/\"\ndf.coalesce(1).write.json(json_sink_path)\n"}, {"cell_type": "code", "execution_count": 27, "id": "f3b655b5-a5b3-4aec-80b2-b3e9f68b4475", "metadata": {"tags": []}, "outputs": [], "source": "\n#  parq file to sink\n"}, {"cell_type": "code", "execution_count": 28, "id": "191294c8-d3a6-4410-882a-ef27cfff935c", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "parq_sink_path=\"gs://class14-files/parq/\"\ndf.coalesce(1).write.parquet(parq_sink_path)"}, {"cell_type": "code", "execution_count": 29, "id": "77825f03-5acb-4b3a-bf4b-f400c26362b9", "metadata": {"tags": []}, "outputs": [], "source": "# load the data to bigquery \n\n\n"}, {"cell_type": "code", "execution_count": 31, "id": "7be2de18-ab96-4965-8dea-8bac2f08b690", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "table_id = \"regular-use-486315.gold_region.pyspark_example\"\n\n(\n    df.write.format(\"bigquery\")\n    .option(\"table\", table_id)\n    .option(\"temporaryGcsBucket\", \"class14-files\")  # bucket name only\n    .mode(\"overwrite\")\n    .save()\n)\n"}, {"cell_type": "code", "execution_count": 32, "id": "8cb6aa5e-6027-4cc8-af17-ea25b78f21e6", "metadata": {"tags": []}, "outputs": [], "source": "#b load data to sql \n\n"}, {"cell_type": "code", "execution_count": null, "id": "f04dc800-bd84-44b3-9676-3358e9e778f6", "metadata": {}, "outputs": [], "source": "mysql_properties = {\n    \"driver\": \"com.mysql.cj.jdbc.Driver\",\n    \"url\": \"jdbc:mysql://<HOST-IP>:3306/<DB-NAME>\",\n    \"user\": \"mysql\",\n    \"password\": \"mypass\",\n    \"table\": \"pysparkTable\"\n}\n\n(\n    df.write\n    .format(\"jdbc\")\n    .options(**mysql_properties)\n    .mode(\"overwrite\")\n    .save()\n)\n"}, {"cell_type": "code", "execution_count": 33, "id": "068ef470-af2d-4f98-8c47-0256f20f0c42", "metadata": {"tags": []}, "outputs": [], "source": "# transformation      ----------------------------------------------------  ------------"}, {"cell_type": "code", "execution_count": null, "id": "7fed372d-0224-478a-9298-f1dc9ef64a2d", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "b3e3e1be-2bae-4d86-9ed0-1a61bbf00769", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "17498233-442a-4e2f-99c9-4c203884c52f", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 36, "id": "f78f9e07-8dd3-426b-8289-195cfd028e86", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-----+---------------+----------+----------+------------+\n|empId|       emp_name|emp_gender|emp_salary|emp_location|\n+-----+---------------+----------+----------+------------+\n|    1|       John Doe|      Male|   60000.0|         USA|\n|    2|     Jane Smith|    Female|   55000.0|      Canada|\n|    3|  Alice Johnson|    Female|   65000.0|          UK|\n|    4|   Bob Williams|      Male|   62000.0|   Australia|\n|    5|      Eve Davis|    Female|   70000.0|       India|\n|    5|      Eve Davis|    Female|   70000.0|       India|\n|    6|  Charlie Brown|      Male|   58000.0|     Germany|\n|    7|   Diana Miller|    Female|   60000.0|      France|\n|    8|  Frank Johnson|      Male|   62000.0|       Spain|\n|    9|   Grace Wilson|    Female|   54000.0|       Italy|\n|   10|    Henry Davis|      Male|   68000.0|       Japan|\n|   11|   Isabel Clark|    Female|   59000.0|      Brazil|\n|   12|    Jack Turner|      Male|   63000.0|      Mexico|\n|   13|Katherine White|    Female|   67000.0|South Africa|\n|   14|   Louis Harris|      Male|   56000.0|      Russia|\n|   15|        Mia Lee|    Female|   61000.0|       China|\n+-----+---------------+----------+----------+------------+\n\n"}], "source": "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType\n\nemp_data = [\n    (1, \"John Doe\", \"Male\", 60000.0, \"USA\"),\n    (2, \"Jane Smith\", \"Female\", 55000.0, \"Canada\"),\n    (3, \"Alice Johnson\", \"Female\", 65000.0, \"UK\"),\n    (4, \"Bob Williams\", \"Male\", 62000.0, \"Australia\"),\n    (5, \"Eve Davis\", \"Female\", 70000.0, \"India\"),\n    (5, \"Eve Davis\", \"Female\", 70000.0, \"India\"),\n    (6, \"Charlie Brown\", \"Male\", 58000.0, \"Germany\"),\n    (7, \"Diana Miller\", \"Female\", 60000.0, \"France\"),\n    (8, \"Frank Johnson\", \"Male\", 62000.0, \"Spain\"),\n    (9, \"Grace Wilson\", \"Female\", 54000.0, \"Italy\"),\n    (10, \"Henry Davis\", \"Male\", 68000.0, \"Japan\"),\n    (11, \"Isabel Clark\", \"Female\", 59000.0, \"Brazil\"),\n    (12, \"Jack Turner\", \"Male\", 63000.0, \"Mexico\"),\n    (13, \"Katherine White\", \"Female\", 67000.0, \"South Africa\"),\n    (14, \"Louis Harris\", \"Male\", 56000.0, \"Russia\"),\n    (15, \"Mia Lee\", \"Female\", 61000.0, \"China\")\n]\n\n# Define schema correctly\nemp_schema = StructType([\n    StructField(\"empId\", IntegerType(), True),\n    StructField(\"emp_name\", StringType(), True),\n    StructField(\"emp_gender\", StringType(), True),\n    StructField(\"emp_salary\", FloatType(), True),\n    StructField(\"emp_location\", StringType(), True)\n])\n\n# Create DataFrame\ndf1 = spark.createDataFrame(emp_data, emp_schema)\n\n# Show DataFrame\ndf1.show()\n"}, {"cell_type": "code", "execution_count": 37, "id": "bc56b5a6-8e49-4bf1-898b-f40d8fa93734", "metadata": {"tags": []}, "outputs": [], "source": "# add two more coloumns \n\n# origin ==> constant coulumn ===> \"india \"\n# tax ====>  12% of salary in new column  as salary "}, {"cell_type": "code", "execution_count": 48, "id": "3301eb17-2fa9-40ce-ab76-30868750a99f", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+-------------+----------+----------+------------+------+------+\n|empId|     emp_name|emp_gender|emp_salary|emp_location|origin|   tax|\n+-----+-------------+----------+----------+------------+------+------+\n|    1|     John Doe|      Male|   60000.0|         USA| India|7200.0|\n|    2|   Jane Smith|    Female|   55000.0|      Canada| India|6600.0|\n|    3|Alice Johnson|    Female|   65000.0|          UK| India|7800.0|\n|    4| Bob Williams|      Male|   62000.0|   Australia| India|7440.0|\n|    5|    Eve Davis|    Female|   70000.0|       India| India|8400.0|\n+-----+-------------+----------+----------+------------+------+------+\nonly showing top 5 rows\n\n"}], "source": "\n# withColumn \n# here lit is for constant column \nfrom pyspark.sql.functions import lit \n\ndf2=df1.withColumn(\"origin\",lit(\"India\"))\\\n       .withColumn(\"tax\", 0.12*df1.emp_salary) ######.tax value * dataframe.columnname\ndf2.show(5)"}, {"cell_type": "code", "execution_count": 55, "id": "30911f08-90ea-4b17-ad64-289c4bfe408f", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+---------------+----------+\n|empId|       emp_name|emp_gender|\n+-----+---------------+----------+\n|    1|       John Doe|      Male|\n|    2|     Jane Smith|    Female|\n|    3|  Alice Johnson|    Female|\n|    4|   Bob Williams|      Male|\n|    5|      Eve Davis|    Female|\n|    5|      Eve Davis|    Female|\n|    6|  Charlie Brown|      Male|\n|    7|   Diana Miller|    Female|\n|    8|  Frank Johnson|      Male|\n|    9|   Grace Wilson|    Female|\n|   10|    Henry Davis|      Male|\n|   11|   Isabel Clark|    Female|\n|   12|    Jack Turner|      Male|\n|   13|Katherine White|    Female|\n|   14|   Louis Harris|      Male|\n|   15|        Mia Lee|    Female|\n+-----+---------------+----------+\n\n"}], "source": "# to specific perticular functions \n\n# for column and row operation we have to import col \n\nfrom pyspark.sql.functions import col \n\ndf1.select(\"empId\",\"emp_name\",col(\"emp_gender\")).show()"}, {"cell_type": "code", "execution_count": 56, "id": "9831f190-4247-4faf-8066-41c0339c1f5f", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+---------------+----------+----------+------------+----------+-------+\n|empId|       emp_name|emp_gender|emp_salary|emp_location|emp_origin|emp_tax|\n+-----+---------------+----------+----------+------------+----------+-------+\n|    1|       John Doe|      Male|   60000.0|         USA|     India| 7200.0|\n|    2|     Jane Smith|    Female|   55000.0|      Canada|     India| 6600.0|\n|    3|  Alice Johnson|    Female|   65000.0|          UK|     India| 7800.0|\n|    4|   Bob Williams|      Male|   62000.0|   Australia|     India| 7440.0|\n|    5|      Eve Davis|    Female|   70000.0|       India|     India| 8400.0|\n|    5|      Eve Davis|    Female|   70000.0|       India|     India| 8400.0|\n|    6|  Charlie Brown|      Male|   58000.0|     Germany|     India| 6960.0|\n|    7|   Diana Miller|    Female|   60000.0|      France|     India| 7200.0|\n|    8|  Frank Johnson|      Male|   62000.0|       Spain|     India| 7440.0|\n|    9|   Grace Wilson|    Female|   54000.0|       Italy|     India| 6480.0|\n|   10|    Henry Davis|      Male|   68000.0|       Japan|     India| 8160.0|\n|   11|   Isabel Clark|    Female|   59000.0|      Brazil|     India| 7080.0|\n|   12|    Jack Turner|      Male|   63000.0|      Mexico|     India| 7560.0|\n|   13|Katherine White|    Female|   67000.0|South Africa|     India| 8040.0|\n|   14|   Louis Harris|      Male|   56000.0|      Russia|     India| 6720.0|\n|   15|        Mia Lee|    Female|   61000.0|       China|     India| 7320.0|\n+-----+---------------+----------+----------+------------+----------+-------+\n\n"}], "source": "# rename the origin to emo_origin \n\n#withColumnRenamed()\n\ndf2=df2.withColumnRenamed(\"origin\",\"emp_origin\")\\\n       .withColumnRenamed(\"tax\",\"emp_tax\")\n\ndf2.show()"}, {"cell_type": "code", "execution_count": 60, "id": "3f178c5b-eee9-4687-86b7-3e4f3b4ce439", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+---------------+----------+----------+------------+----------+-------+\n|empId|       emp_name|emp_gender|emp_salary|emp_location|emp_origin|emp_tax|\n+-----+---------------+----------+----------+------------+----------+-------+\n|    1|       John Doe|         m|   60000.0|         USA|     India| 7200.0|\n|    2|     Jane Smith|         f|   55000.0|      Canada|     India| 6600.0|\n|    3|  Alice Johnson|         f|   65000.0|          UK|     India| 7800.0|\n|    4|   Bob Williams|         m|   62000.0|   Australia|     India| 7440.0|\n|    5|      Eve Davis|         f|   70000.0|       India|     India| 8400.0|\n|    5|      Eve Davis|         f|   70000.0|       India|     India| 8400.0|\n|    6|  Charlie Brown|         m|   58000.0|     Germany|     India| 6960.0|\n|    7|   Diana Miller|         f|   60000.0|      France|     India| 7200.0|\n|    8|  Frank Johnson|         m|   62000.0|       Spain|     India| 7440.0|\n|    9|   Grace Wilson|         f|   54000.0|       Italy|     India| 6480.0|\n|   10|    Henry Davis|         m|   68000.0|       Japan|     India| 8160.0|\n|   11|   Isabel Clark|         f|   59000.0|      Brazil|     India| 7080.0|\n|   12|    Jack Turner|         m|   63000.0|      Mexico|     India| 7560.0|\n|   13|Katherine White|         f|   67000.0|South Africa|     India| 8040.0|\n|   14|   Louis Harris|         m|   56000.0|      Russia|     India| 6720.0|\n|   15|        Mia Lee|         f|   61000.0|       China|     India| 7320.0|\n+-----+---------------+----------+----------+------------+----------+-------+\n\n"}], "source": "# case conditions # where #  otherwise \n\n# here m =1\n# f=2\n\nfrom pyspark.sql.functions import when\n\ndf3=df2.select(\"empId\",\"emp_name\",when(df2.emp_gender==\"Male\",\"m\").when(df2.emp_gender==\"Female\",\"f\").otherwise(\"u\").alias(\"emp_gender\"),\n                                                                                                                            \"emp_salary\",\n                                                                                                                            \"emp_location\",\n                                                                                                                            \"emp_origin\",\n                                                                                                                            \"emp_tax\")\ndf3.show()\n\n"}, {"cell_type": "code", "execution_count": 62, "id": "ac48c001-fa5e-4e63-88e1-4885ef143b4c", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+---------------+----------+----------+------------+----------+-------+\n|empId|       emp_name|emp_gender|emp_salary|emp_location|emp_origin|emp_tax|\n+-----+---------------+----------+----------+------------+----------+-------+\n|    5|      Eve Davis|         f|   70000.0|       India|     India| 8400.0|\n|    5|      Eve Davis|         f|   70000.0|       India|     India| 8400.0|\n|   10|    Henry Davis|         m|   68000.0|       Japan|     India| 8160.0|\n|   13|Katherine White|         f|   67000.0|South Africa|     India| 8040.0|\n|    3|  Alice Johnson|         f|   65000.0|          UK|     India| 7800.0|\n|   12|    Jack Turner|         m|   63000.0|      Mexico|     India| 7560.0|\n|    4|   Bob Williams|         m|   62000.0|   Australia|     India| 7440.0|\n|    8|  Frank Johnson|         m|   62000.0|       Spain|     India| 7440.0|\n|   15|        Mia Lee|         f|   61000.0|       China|     India| 7320.0|\n|    1|       John Doe|         m|   60000.0|         USA|     India| 7200.0|\n|    7|   Diana Miller|         f|   60000.0|      France|     India| 7200.0|\n|   11|   Isabel Clark|         f|   59000.0|      Brazil|     India| 7080.0|\n|    6|  Charlie Brown|         m|   58000.0|     Germany|     India| 6960.0|\n|   14|   Louis Harris|         m|   56000.0|      Russia|     India| 6720.0|\n|    2|     Jane Smith|         f|   55000.0|      Canada|     India| 6600.0|\n|    9|   Grace Wilson|         f|   54000.0|       Italy|     India| 6480.0|\n+-----+---------------+----------+----------+------------+----------+-------+\n\n"}], "source": "# in salary i want to  sort the column \n\n#  orderby() and sort() can be use \n\n\n\ndf3.sort(df3.emp_salary.desc()).show()"}, {"cell_type": "code", "execution_count": 64, "id": "7ae066a2-1637-495b-857c-0904fe3d9030", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+---------------+----------+----------+------------+----------+-------+\n|empId|       emp_name|emp_gender|emp_salary|emp_location|emp_origin|emp_tax|\n+-----+---------------+----------+----------+------------+----------+-------+\n|    5|      Eve Davis|         f|   70000.0|       India|     India| 8400.0|\n|   10|    Henry Davis|         m|   68000.0|       Japan|     India| 8160.0|\n|   13|Katherine White|         f|   67000.0|South Africa|     India| 8040.0|\n|    3|  Alice Johnson|         f|   65000.0|          UK|     India| 7800.0|\n|   12|    Jack Turner|         m|   63000.0|      Mexico|     India| 7560.0|\n|    4|   Bob Williams|         m|   62000.0|   Australia|     India| 7440.0|\n|    8|  Frank Johnson|         m|   62000.0|       Spain|     India| 7440.0|\n|   15|        Mia Lee|         f|   61000.0|       China|     India| 7320.0|\n|    7|   Diana Miller|         f|   60000.0|      France|     India| 7200.0|\n|    1|       John Doe|         m|   60000.0|         USA|     India| 7200.0|\n|   11|   Isabel Clark|         f|   59000.0|      Brazil|     India| 7080.0|\n|    6|  Charlie Brown|         m|   58000.0|     Germany|     India| 6960.0|\n|   14|   Louis Harris|         m|   56000.0|      Russia|     India| 6720.0|\n|    2|     Jane Smith|         f|   55000.0|      Canada|     India| 6600.0|\n|    9|   Grace Wilson|         f|   54000.0|       Italy|     India| 6480.0|\n+-----+---------------+----------+----------+------------+----------+-------+\n\n"}], "source": "# to drop duplicates \n\n#dropDuplicates()\n\ndf4=df3.dropDuplicates().sort(df3.emp_salary.desc())\ndf4.show()"}, {"cell_type": "code", "execution_count": 67, "id": "0427a39b-227b-4f3d-8a3c-84142809c1e9", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+--------+----------+----------+------------+----------+-------+\n|empId|emp_name|emp_gender|emp_salary|emp_location|emp_origin|emp_tax|\n+-----+--------+----------+----------+------------+----------+-------+\n+-----+--------+----------+----------+------------+----------+-------+\n\n"}], "source": "# where () or filter() used to filter the data \n\n(df4.filter((df4.emp_salary >=55000)&(df4.emp_gender==\"F\")&(df4.emp_name.like(\"%e\"))) .show())"}, {"cell_type": "code", "execution_count": 68, "id": "bd932161-cefa-4ab4-996b-7e43a43df089", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+--------+----------+----------+------------+----------+-------+\n|empId|emp_name|emp_gender|emp_salary|emp_location|emp_origin|emp_tax|\n+-----+--------+----------+----------+------------+----------+-------+\n+-----+--------+----------+----------+------------+----------+-------+\n\n"}], "source": "df4.filter(\n    (col(\"emp_salary\") >= 55000) &\n    (col(\"emp_gender\") == \"Female\") &\n    (col(\"emp_name\").like(\"%e\"))\n).show()\n"}, {"cell_type": "code", "execution_count": null, "id": "3abb304c-7144-4921-ab94-be1ea11b827d", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}