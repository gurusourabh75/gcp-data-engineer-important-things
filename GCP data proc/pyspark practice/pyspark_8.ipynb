{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b4a9b2d",
   "metadata": {},
   "source": [
    "1. Working with String Data\n",
    "1. Case When, Regex_Replace etc\n",
    "2. Working with Dates\n",
    "1. to_date, current_date, current_timestamp\n",
    "3. Working with NULL values\n",
    "1. nvl, na.drop, na.fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5d138ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=(SparkSession.builder.appName(\"sparkintroduction\").getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70ddb456",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_data = [\n",
    "    [\"001\",\"101\",\"John Doe\",\"30\",\"Male\",\"50000\",\"2015-01-01\"],\n",
    "    [\"002\",\"101\",\"Jane Smith\",\"25\",\"Female\",\"45000\",\"2016-02-15\"],\n",
    "    [\"003\",\"102\",\"Bob Brown\",\"35\",\"Male\",\"55000\",\"2014-05-01\"],\n",
    "    [\"004\",\"102\",\"Alice Lee\",\"28\",\"Female\",\"48000\",\"2017-09-30\"],\n",
    "    [\"005\",\"103\",\"Jack Chan\",\"40\",\"Male\",\"60000\",\"2013-04-01\"],\n",
    "    [\"006\",\"103\",\"Jill Wong\",\"32\",\"Female\",\"52000\",\"2018-07-01\"],\n",
    "    [\"007\",\"101\",\"James Johnson\",\"42\",\"Male\",\"70000\",\"2012-03-15\"],\n",
    "    [\"008\",\"102\",\"Kate Kim\",\"29\",\"Female\",\"51000\",\"2019-10-01\"],\n",
    "    [\"009\",\"103\",\"Tom Tan\",\"33\",\"Male\",\"58000\",\"2016-06-01\"],\n",
    "    [\"010\",\"104\",\"Lisa Lee\",\"27\",\"Female\",\"47000\",\"2018-08-01\"],\n",
    "    [\"011\",\"104\",\"David Park\",\"38\",\"Male\",\"65000\",\"2015-11-01\"],\n",
    "    [\"012\",\"105\",\"Susan Chen\",\"31\",\"Female\",\"54000\",\"2017-02-15\"],\n",
    "    [\"013\",\"106\",\"Brian Kim\",\"45\",\"Male\",\"75000\",\"2011-07-01\"],\n",
    "    [\"014\",\"107\",\"Emily Lee\",\"26\",\"Female\",\"46000\",\"2019-01-01\"],\n",
    "    [\"015\",\"106\",\"Michael Lee\",\"37\",\"Male\",\"63000\",\"2014-09-30\"],\n",
    "    [\"016\",\"107\",\"Kelly Zhang\",\"30\",\"Female\",\"49000\",\"2018-04-01\"],\n",
    "    [\"017\",\"105\",\"George Wang\",\"34\",\"Male\",\"57000\",\"2016-03-15\"],\n",
    "    [\"018\",\"104\",\"Nancy Liu\",\"29\",\" \",\"50000\",\"2017-06-01\"],\n",
    "    [\"019\",\"103\",\"Steven Chen\",\"36\",\"Male\",\"62000\",\"2015-08-01\"],\n",
    "    [\"020\",\"102\",\"Grace Kim\",\"32\",\"Female\",\"53000\",\"2018-11-01\"]\n",
    "]\n",
    "\n",
    "emp_schema = \"employee_id string, department_id string, name string, age string, gender string, salary string, hire_date string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa97e8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
      "|        018|          104|    Nancy Liu| 29|      | 50000|2017-06-01|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n",
      "+-----------+-------------+-------------+---+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp=spark.createDataFrame(data=emp_data, schema=emp_schema)\n",
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fa7d000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+----------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|new_gender|\n",
      "+-----------+-------------+-------------+---+------+------+----------+----------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|         M|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|         F|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|         M|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|         F|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|         M|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|         F|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|         M|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|         F|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|         M|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|         F|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|         M|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|         F|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|         M|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|         F|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|         M|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|         F|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|         M|\n",
      "|        018|          104|    Nancy Liu| 29|      | 50000|2017-06-01|      NULL|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|         M|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|         F|\n",
      "+-----------+-------------+-------------+---+------+------+----------+----------+\n",
      "\n",
      "+-----------+-------------+-------------+---+------+------+----------+----------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|new_gender|\n",
      "+-----------+-------------+-------------+---+------+------+----------+----------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|         M|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|         F|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|         M|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|         F|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|         M|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|         F|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|         M|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|         F|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|         M|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|         F|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|         M|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|         F|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|         M|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|         F|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|         M|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|         F|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|         M|\n",
      "|        018|          104|    Nancy Liu| 29|      | 50000|2017-06-01|      NULL|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|         M|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|         F|\n",
      "+-----------+-------------+-------------+---+------+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Case When\n",
    "# select employee_id, name, age, salary, gender,\n",
    "# case when gender = 'Male' then 'M' when gender = 'Female' then 'F' else null end as new gender, hire_date from\n",
    "\n",
    "from pyspark.sql.functions import when, col ,expr\n",
    "\n",
    "emp_gender_fixed =emp.withColumn(\"new_gender\" ,when(col(\"gender\") == 'Male','M')\n",
    "                                .when(col(\"gender\") =='Female','F')\n",
    "                                .otherwise(None))\n",
    "emp_gender_fixed.show()\n",
    "\n",
    "\n",
    "emp_gender_fixed_1=emp.withColumn(\"new_gender\", expr(\"case when gender = 'Male' then 'M' when gender = 'Female' then 'F' else null end \"))\n",
    "emp_gender_fixed_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f66bf155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+----------+-------------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|new_gender|     new_name|\n",
      "+-----------+-------------+-------------+---+------+------+----------+----------+-------------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|         M|     zohn Doe|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|         F|   zane Smith|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|         M|    Bob Brown|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|         F|    Alice Lee|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|         M|    zack Chan|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|         F|    zill Wong|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|         M|zames zohnson|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|         F|     Kate Kim|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|         M|      Tom Tan|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|         F|     Lisa Lee|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|         M|   David Park|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|         F|   Susan Chen|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|         M|    Brian Kim|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|         F|    Emily Lee|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|         M|  Michael Lee|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|         F|  Kelly Zhang|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|         M|  George Wang|\n",
      "|        018|          104|    Nancy Liu| 29|      | 50000|2017-06-01|      NULL|    Nancy Liu|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|         M|  Steven Chen|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|         F|    Grace Kim|\n",
      "+-----------+-------------+-------------+---+------+------+----------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace in Strings\n",
    "# select empLoyee_id, name, replace(name, 'J', 'Z') as new_name, age, salary, gender, new_gender, hire_date from emp_gender_fixed\n",
    "\n",
    "# important function   \"\"\"\"\"\"\"\"\"\".   regexp_replace. \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "emp_name_fixed = emp_gender_fixed_1.withColumn(\"new_name\",regexp_replace(col(\"name\"),\"J\",\"z\"))\n",
    "emp_name_fixed.show()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c30e08ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+-------------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|     new_name|\n",
      "+-----------+-------------+-------------+---+------+------+----------+-------------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|     zohn Doe|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-01-15|   zane Smith|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-01-01|    Bob Brown|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-01-30|    Alice Lee|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-01-01|    zack Chan|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-01-01|    zill Wong|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-01-15|zames zohnson|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-01-01|     Kate Kim|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-01-01|      Tom Tan|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-01-01|     Lisa Lee|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-01-01|   David Park|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-01-15|   Susan Chen|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-01-01|    Brian Kim|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|    Emily Lee|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-01-30|  Michael Lee|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-01-01|  Kelly Zhang|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-01-15|  George Wang|\n",
      "|        018|          104|    Nancy Liu| 29|      | 50000|2017-01-01|    Nancy Liu|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-01-01|  Steven Chen|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-01-01|    Grace Kim|\n",
      "+-----------+-------------+-------------+---+------+------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert Date\n",
    "# select *, to_ date(hire_date, 'YYY-MM-DD') as hire_date from emp_name fixed\n",
    "\n",
    "from pyspark.sql.functions import to_date \n",
    "\n",
    "emp_date_fix =emp_name_fixed.withColumn(\"hire_date\",to_date(col(\"hire_date\"),\"yyyy-mm-dd\"))\n",
    "emp_date_fix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d135835f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: string (nullable = true)\n",
      " |-- department_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- new_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_date_fix.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ccad230b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+-------------+----------+--------------------------+\n",
      "|employee_id|department_id|name         |age|gender|salary|hire_date |new_name     |data_now  |timestamp_now             |\n",
      "+-----------+-------------+-------------+---+------+------+----------+-------------+----------+--------------------------+\n",
      "|001        |101          |John Doe     |30 |Male  |50000 |2015-01-01|zohn Doe     |2026-02-25|2026-02-25 22:34:48.848685|\n",
      "|002        |101          |Jane Smith   |25 |Female|45000 |2016-01-15|zane Smith   |2026-02-25|2026-02-25 22:34:48.848685|\n",
      "|003        |102          |Bob Brown    |35 |Male  |55000 |2014-01-01|Bob Brown    |2026-02-25|2026-02-25 22:34:48.848685|\n",
      "|004        |102          |Alice Lee    |28 |Female|48000 |2017-01-30|Alice Lee    |2026-02-25|2026-02-25 22:34:48.848685|\n",
      "|005        |103          |Jack Chan    |40 |Male  |60000 |2013-01-01|zack Chan    |2026-02-25|2026-02-25 22:34:48.848685|\n",
      "|006        |103          |Jill Wong    |32 |Female|52000 |2018-01-01|zill Wong    |2026-02-25|2026-02-25 22:34:48.848685|\n",
      "|007        |101          |James Johnson|42 |Male  |70000 |2012-01-15|zames zohnson|2026-02-25|2026-02-25 22:34:48.848685|\n",
      "|008        |102          |Kate Kim     |29 |Female|51000 |2019-01-01|Kate Kim     |2026-02-25|2026-02-25 22:34:48.848685|\n",
      "|009        |103          |Tom Tan      |33 |Male  |58000 |2016-01-01|Tom Tan      |2026-02-25|2026-02-25 22:34:48.848685|\n",
      "|010        |104          |Lisa Lee     |27 |Female|47000 |2018-01-01|Lisa Lee     |2026-02-25|2026-02-25 22:34:48.848685|\n",
      "|011        |104          |David Park   |38 |Male  |65000 |2015-01-01|David Park   |2026-02-25|2026-02-25 22:34:48.848685|\n",
      "|012        |105          |Susan Chen   |31 |Female|54000 |2017-01-15|Susan Chen   |2026-02-25|2026-02-25 22:34:48.848685|\n",
      "|013        |106          |Brian Kim    |45 |Male  |75000 |2011-01-01|Brian Kim    |2026-02-25|2026-02-25 22:34:48.848685|\n",
      "|014        |107          |Emily Lee    |26 |Female|46000 |2019-01-01|Emily Lee    |2026-02-25|2026-02-25 22:34:48.848685|\n",
      "|015        |106          |Michael Lee  |37 |Male  |63000 |2014-01-30|Michael Lee  |2026-02-25|2026-02-25 22:34:48.848685|\n",
      "|016        |107          |Kelly Zhang  |30 |Female|49000 |2018-01-01|Kelly Zhang  |2026-02-25|2026-02-25 22:34:48.848685|\n",
      "|017        |105          |George Wang  |34 |Male  |57000 |2016-01-15|George Wang  |2026-02-25|2026-02-25 22:34:48.848685|\n",
      "|018        |104          |Nancy Liu    |29 |      |50000 |2017-01-01|Nancy Liu    |2026-02-25|2026-02-25 22:34:48.848685|\n",
      "|019        |103          |Steven Chen  |36 |Male  |62000 |2015-01-01|Steven Chen  |2026-02-25|2026-02-25 22:34:48.848685|\n",
      "|020        |102          |Grace Kim    |32 |Female|53000 |2018-01-01|Grace Kim    |2026-02-25|2026-02-25 22:34:48.848685|\n",
      "+-----------+-------------+-------------+---+------+------+----------+-------------+----------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add Date Columns\n",
    "# Add current_date, current_timestamp, extract year from hire_date\n",
    "\n",
    "from pyspark.sql.functions import current_date,current_timestamp,year\n",
    "\n",
    "emp_dated=emp_date_fix.withColumn(\"data_now\",current_date()).withColumn(\"timestamp_now\",current_timestamp())\n",
    "emp_dated.show(truncate=False)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccd1b5e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2aa25e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+-------------+----------+--------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|     new_name|  data_now|       timestamp_now|\n",
      "+-----------+-------------+-------------+---+------+------+----------+-------------+----------+--------------------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|     zohn Doe|2026-02-25|2026-02-25 22:36:...|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-01-15|   zane Smith|2026-02-25|2026-02-25 22:36:...|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-01-01|    Bob Brown|2026-02-25|2026-02-25 22:36:...|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-01-30|    Alice Lee|2026-02-25|2026-02-25 22:36:...|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-01-01|    zack Chan|2026-02-25|2026-02-25 22:36:...|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-01-01|    zill Wong|2026-02-25|2026-02-25 22:36:...|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-01-15|zames zohnson|2026-02-25|2026-02-25 22:36:...|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-01-01|     Kate Kim|2026-02-25|2026-02-25 22:36:...|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-01-01|      Tom Tan|2026-02-25|2026-02-25 22:36:...|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-01-01|     Lisa Lee|2026-02-25|2026-02-25 22:36:...|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-01-01|   David Park|2026-02-25|2026-02-25 22:36:...|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-01-15|   Susan Chen|2026-02-25|2026-02-25 22:36:...|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-01-01|    Brian Kim|2026-02-25|2026-02-25 22:36:...|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|    Emily Lee|2026-02-25|2026-02-25 22:36:...|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-01-30|  Michael Lee|2026-02-25|2026-02-25 22:36:...|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-01-01|  Kelly Zhang|2026-02-25|2026-02-25 22:36:...|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-01-15|  George Wang|2026-02-25|2026-02-25 22:36:...|\n",
      "|        018|          104|    Nancy Liu| 29|      | 50000|2017-01-01|    Nancy Liu|2026-02-25|2026-02-25 22:36:...|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-01-01|  Steven Chen|2026-02-25|2026-02-25 22:36:...|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-01-01|    Grace Kim|2026-02-25|2026-02-25 22:36:...|\n",
      "+-----------+-------------+-------------+---+------+------+----------+-------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop null genders record \n",
    "\n",
    "emp_1=emp_dated.na.drop()\n",
    "emp_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a4229f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2026-02-25 22:41:25.354\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `new_gender` cannot be resolved. Did you mean one of the following? [`gender`, `new_name`, `age`, `data_now`, `hire_date`]. SQLSTATE: 42703\", \"context\": {\"file\": \"line 5 in cell [30]\", \"line\": \"\", \"fragment\": \"col\", \"errorClass\": \"UNRESOLVED_COLUMN.WITH_SUGGESTION\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o133.withColumn.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `new_gender` cannot be resolved. Did you mean one of the following? [`gender`, `new_name`, `age`, `data_now`, `hire_date`]. SQLSTATE: 42703;\\n'Project [employee_id#197, department_id#198, name#199, age#200, gender#201, salary#202, hire_date#304, new_name#278, data_now#330, timestamp_now#331, 'coalesce('new_gender, O) AS new_gender#516]\\n+- Project [employee_id#197, department_id#198, name#199, age#200, gender#201, salary#202, hire_date#304, new_name#278, data_now#330, current_timestamp() AS timestamp_now#331]\\n   +- Project [employee_id#197, department_id#198, name#199, age#200, gender#201, salary#202, hire_date#304, new_name#278, current_date(Some(Asia/Kolkata)) AS data_now#330]\\n      +- Project [employee_id#197, department_id#198, name#199, age#200, gender#201, salary#202, to_date(hire_date#203, Some(yyyy-mm-dd), Some(Asia/Kolkata), true) AS hire_date#304, new_name#278]\\n         +- Project [employee_id#197, department_id#198, name#199, age#200, gender#201, salary#202, hire_date#203, regexp_replace(name#199, J, z, 1) AS new_name#278]\\n            +- LogicalRDD [employee_id#197, department_id#198, name#199, age#200, gender#201, salary#202, hire_date#203], false\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2263)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumns(Dataset.scala:1283)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumns(Dataset.scala:232)\\n\\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2187)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumn(Dataset.scala:1819)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:842)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 23 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/opt/anaconda3/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/opt/anaconda3/lib/python3.13/site-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `new_gender` cannot be resolved. Did you mean one of the following? [`gender`, `new_name`, `age`, `data_now`, `hire_date`]. SQLSTATE: 42703;\n'Project [employee_id#197, department_id#198, name#199, age#200, gender#201, salary#202, hire_date#304, new_name#278, data_now#330, timestamp_now#331, 'coalesce('new_gender, O) AS new_gender#516]\n+- Project [employee_id#197, department_id#198, name#199, age#200, gender#201, salary#202, hire_date#304, new_name#278, data_now#330, current_timestamp() AS timestamp_now#331]\n   +- Project [employee_id#197, department_id#198, name#199, age#200, gender#201, salary#202, hire_date#304, new_name#278, current_date(Some(Asia/Kolkata)) AS data_now#330]\n      +- Project [employee_id#197, department_id#198, name#199, age#200, gender#201, salary#202, to_date(hire_date#203, Some(yyyy-mm-dd), Some(Asia/Kolkata), true) AS hire_date#304, new_name#278]\n         +- Project [employee_id#197, department_id#198, name#199, age#200, gender#201, salary#202, hire_date#203, regexp_replace(name#199, J, z, 1) AS new_name#278]\n            +- LogicalRDD [employee_id#197, department_id#198, name#199, age#200, gender#201, salary#202, hire_date#203], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fix NulL values\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# select *, nul ('new_ gender\", '0') as new gender from emp_dated\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m coalesce, lit \n\u001b[0;32m----> 5\u001b[0m emp_null_df \u001b[38;5;241m=\u001b[39memp_dated\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_gender\u001b[39m\u001b[38;5;124m\"\u001b[39m,coalesce(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_gender\u001b[39m\u001b[38;5;124m\"\u001b[39m),lit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m\"\u001b[39m) ))\n\u001b[1;32m      6\u001b[0m emp_null_df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pyspark/sql/classic/dataframe.py:1623\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m   1619\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1620\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1621\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   1622\u001b[0m     )\n\u001b[0;32m-> 1623\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mwithColumn(colName, col\u001b[38;5;241m.\u001b[39m_jc), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1363\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    284\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `new_gender` cannot be resolved. Did you mean one of the following? [`gender`, `new_name`, `age`, `data_now`, `hire_date`]. SQLSTATE: 42703;\n'Project [employee_id#197, department_id#198, name#199, age#200, gender#201, salary#202, hire_date#304, new_name#278, data_now#330, timestamp_now#331, 'coalesce('new_gender, O) AS new_gender#516]\n+- Project [employee_id#197, department_id#198, name#199, age#200, gender#201, salary#202, hire_date#304, new_name#278, data_now#330, current_timestamp() AS timestamp_now#331]\n   +- Project [employee_id#197, department_id#198, name#199, age#200, gender#201, salary#202, hire_date#304, new_name#278, current_date(Some(Asia/Kolkata)) AS data_now#330]\n      +- Project [employee_id#197, department_id#198, name#199, age#200, gender#201, salary#202, to_date(hire_date#203, Some(yyyy-mm-dd), Some(Asia/Kolkata), true) AS hire_date#304, new_name#278]\n         +- Project [employee_id#197, department_id#198, name#199, age#200, gender#201, salary#202, hire_date#203, regexp_replace(name#199, J, z, 1) AS new_name#278]\n            +- LogicalRDD [employee_id#197, department_id#198, name#199, age#200, gender#201, salary#202, hire_date#203], false\n"
     ]
    }
   ],
   "source": [
    "# Fix NULL values for gender\n",
    "# Add new_gender column if not present, then fill nulls with 'O'\n",
    "\n",
    "from pyspark.sql.functions import coalesce, lit, when, col\n",
    "\n",
    "emp_null_df = emp_dated.withColumn(\n",
    "\t\"new_gender\",\n",
    "\tcoalesce(\n",
    "\t\twhen(col(\"gender\") == 'Male', 'M')\n",
    "\t\t.when(col(\"gender\") == 'Female', 'F')\n",
    "\t\t.otherwise(None),\n",
    "\t\tlit(\"O\")\n",
    "\t)\n",
    ")\n",
    "emp_null_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5421d89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+-------------+----------+--------------------+\n",
      "|employee_id|department_id|         name|age|gender|salary| hire_date|     new_name|  data_now|       timestamp_now|\n",
      "+-----------+-------------+-------------+---+------+------+----------+-------------+----------+--------------------+\n",
      "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|     zohn Doe|2026-02-25|2026-02-25 22:44:...|\n",
      "|        002|          101|   Jane Smith| 25|Female| 45000|2016-01-15|   zane Smith|2026-02-25|2026-02-25 22:44:...|\n",
      "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-01-01|    Bob Brown|2026-02-25|2026-02-25 22:44:...|\n",
      "|        004|          102|    Alice Lee| 28|Female| 48000|2017-01-30|    Alice Lee|2026-02-25|2026-02-25 22:44:...|\n",
      "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-01-01|    zack Chan|2026-02-25|2026-02-25 22:44:...|\n",
      "|        006|          103|    Jill Wong| 32|Female| 52000|2018-01-01|    zill Wong|2026-02-25|2026-02-25 22:44:...|\n",
      "|        007|          101|James Johnson| 42|  Male| 70000|2012-01-15|zames zohnson|2026-02-25|2026-02-25 22:44:...|\n",
      "|        008|          102|     Kate Kim| 29|Female| 51000|2019-01-01|     Kate Kim|2026-02-25|2026-02-25 22:44:...|\n",
      "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-01-01|      Tom Tan|2026-02-25|2026-02-25 22:44:...|\n",
      "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-01-01|     Lisa Lee|2026-02-25|2026-02-25 22:44:...|\n",
      "|        011|          104|   David Park| 38|  Male| 65000|2015-01-01|   David Park|2026-02-25|2026-02-25 22:44:...|\n",
      "|        012|          105|   Susan Chen| 31|Female| 54000|2017-01-15|   Susan Chen|2026-02-25|2026-02-25 22:44:...|\n",
      "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-01-01|    Brian Kim|2026-02-25|2026-02-25 22:44:...|\n",
      "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|    Emily Lee|2026-02-25|2026-02-25 22:44:...|\n",
      "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-01-30|  Michael Lee|2026-02-25|2026-02-25 22:44:...|\n",
      "|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-01-01|  Kelly Zhang|2026-02-25|2026-02-25 22:44:...|\n",
      "|        017|          105|  George Wang| 34|  Male| 57000|2016-01-15|  George Wang|2026-02-25|2026-02-25 22:44:...|\n",
      "|        018|          104|    Nancy Liu| 29|      | 50000|2017-01-01|    Nancy Liu|2026-02-25|2026-02-25 22:44:...|\n",
      "|        019|          103|  Steven Chen| 36|  Male| 62000|2015-01-01|  Steven Chen|2026-02-25|2026-02-25 22:44:...|\n",
      "|        020|          102|    Grace Kim| 32|Female| 53000|2018-01-01|    Grace Kim|2026-02-25|2026-02-25 22:44:...|\n",
      "+-----------+-------------+-------------+---+------+------+----------+-------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d01f6da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+------+----------+----------+--------------------+\n",
      "|employee_id|department_id|age|salary| hire_date|  data_now|       timestamp_now|\n",
      "+-----------+-------------+---+------+----------+----------+--------------------+\n",
      "|        001|          101| 30| 50000|2015-01-01|2026-02-25|2026-02-25 22:49:...|\n",
      "|        002|          101| 25| 45000|2016-01-15|2026-02-25|2026-02-25 22:49:...|\n",
      "|        003|          102| 35| 55000|2014-01-01|2026-02-25|2026-02-25 22:49:...|\n",
      "|        004|          102| 28| 48000|2017-01-30|2026-02-25|2026-02-25 22:49:...|\n",
      "|        005|          103| 40| 60000|2013-01-01|2026-02-25|2026-02-25 22:49:...|\n",
      "|        006|          103| 32| 52000|2018-01-01|2026-02-25|2026-02-25 22:49:...|\n",
      "|        007|          101| 42| 70000|2012-01-15|2026-02-25|2026-02-25 22:49:...|\n",
      "|        008|          102| 29| 51000|2019-01-01|2026-02-25|2026-02-25 22:49:...|\n",
      "|        009|          103| 33| 58000|2016-01-01|2026-02-25|2026-02-25 22:49:...|\n",
      "|        010|          104| 27| 47000|2018-01-01|2026-02-25|2026-02-25 22:49:...|\n",
      "|        011|          104| 38| 65000|2015-01-01|2026-02-25|2026-02-25 22:49:...|\n",
      "|        012|          105| 31| 54000|2017-01-15|2026-02-25|2026-02-25 22:49:...|\n",
      "|        013|          106| 45| 75000|2011-01-01|2026-02-25|2026-02-25 22:49:...|\n",
      "|        014|          107| 26| 46000|2019-01-01|2026-02-25|2026-02-25 22:49:...|\n",
      "|        015|          106| 37| 63000|2014-01-30|2026-02-25|2026-02-25 22:49:...|\n",
      "|        016|          107| 30| 49000|2018-01-01|2026-02-25|2026-02-25 22:49:...|\n",
      "|        017|          105| 34| 57000|2016-01-15|2026-02-25|2026-02-25 22:49:...|\n",
      "|        018|          104| 29| 50000|2017-01-01|2026-02-25|2026-02-25 22:49:...|\n",
      "|        019|          103| 36| 62000|2015-01-01|2026-02-25|2026-02-25 22:49:...|\n",
      "|        020|          102| 32| 53000|2018-01-01|2026-02-25|2026-02-25 22:49:...|\n",
      "+-----------+-------------+---+------+----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# droping old coloumns name with new columns names  \n",
    "\n",
    "emp_final= emp_final.drop(\"name\",\"gender\").withColumnRenamed(\"new_name\",\"name\").withColumnRenamed(\"new_gender\",\"gender\")\n",
    "emp_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0b78ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### sry i messsed up some where  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
